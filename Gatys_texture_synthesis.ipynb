{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gatys_texture_synthesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpO0fIpzL6aUsRe/3RuZvn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/mva_stochastic_models/blob/master/Gatys_texture_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dDSmyxtLxLk",
        "colab_type": "text"
      },
      "source": [
        "This practical session is based on\n",
        "**[Texture Synthesis Using Convolutional Neural Networks](https://papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks)**\n",
        "by Leon Gatys, Alexander S. Ecker, and Matthias Bethge, NIPS 2015\n",
        "\n",
        "Most of the code is from L. Gatys' repo\n",
        "https://github.com/leongatys/PytorchNeuralStyleTransfer\n",
        "\n",
        "Let us recall the algorithm proposed by Gatys et al.\n",
        "Given an example image $u$ and a random initialization $x_0$, \n",
        "one optimizes the cost function \n",
        "$$\n",
        "F(x) = \\sum_{\\text{for selected layers } L} w_L\\left\\| G^L(x) - G^L(u) \\right\\|^2_F\n",
        "$$\n",
        "where $\\|\\cdot\\|$ is the Frobenius norm and for an image $y$ and a layer index $L$ $G^L(y)$ denotes the Gram matrix of the VGG-19 features at layer $L$:\n",
        "if $V^L(y)$ is the feature response of $y$ at layer $L$ that has spatial size $w\\times h$ and $n$ channels, \n",
        "$$\n",
        "G^L(y) = \\frac{1}{w h}\\sum_{k\\in \\{0,\\dots,w-1\\}\\times\\{0,\\dots,h-1\\}} V^L(y)_k V^L(y)_k^T \\in \\mathbb{R}^{n\\times n}.\n",
        "$$\n",
        "The optimization is done using the L-BFGS algorithm.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6xEGZ7ZxQa_",
        "colab_type": "text"
      },
      "source": [
        "**Exercice:**\n",
        "\n",
        "\n",
        "1.   Go through the code and execute the algorithm.\n",
        "\n",
        "3. Pick one of the following problems:\n",
        "\n",
        "A/ (easy) Observe that for some image the color is inconsistant. Propose and implement a solution to correct the output color distribution.\n",
        "\n",
        "B/ (intermediate) Add a term to the energy that would enforce a consistency with the original Fourier spectrum of each color channel. What is the interest of this approach.\n",
        "What are the textures for which it improves or degrades the quality of the result.\n",
        "\n",
        "C/ (intermediate) Replace $F$ so that the spatial average of ALL the VGG-19 layers is preserved. Compare with the original model. What are the interests of this approach ?\n",
        "\n",
        "D/ (advanced) As proposed in \n",
        "[Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](http://proceedings.mlr.press/v48/ulyanov16.pdf) by Ulyanov et al., ICML 2016,\n",
        "train a generative neural networks using Gatys et al's cost function $F$ as a discriminative network. A PyTorch implementation by [Jorge Gutierrez](https://sites.google.com/view/jorge-gutierrezor/home) can be found [here](https://github.com/JorgeGtz/TextureNets_implementation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URrdAlxELyA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os \n",
        "image_dir = os.getcwd()+'/'\n",
        "model_dir = os.getcwd()+'/'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k9FXffLOrDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get binary files for vgg19 parameters:\n",
        "!wget -c --no-check-certificate https://bethgelab.org/media/uploads/pytorch_models/vgg_conv.pth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxSnoBXaTpLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get some texture images (images are 512x512)\n",
        "!wget -c https://www.idpoisson.fr/galerne/mva/bark1001.png\n",
        "!wget -c https://www.idpoisson.fr/galerne/mva/wall1003.png\n",
        "!wget -c https://www.idpoisson.fr/galerne/mva/wall1029.png\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI9G0rDPOVRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vgg definition that conveniently let's you grab the outputs from any layer\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, pool='max'):\n",
        "        super(VGG, self).__init__()\n",
        "        #vgg modules\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        if pool == 'max':\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pool == 'avg':\n",
        "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            \n",
        "    def forward(self, x, out_keys):\n",
        "        out = {}\n",
        "        out['r11'] = F.relu(self.conv1_1(x))\n",
        "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
        "        out['p1'] = self.pool1(out['r12'])\n",
        "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
        "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
        "        out['p2'] = self.pool2(out['r22'])\n",
        "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
        "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
        "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
        "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
        "        out['p3'] = self.pool3(out['r34'])\n",
        "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
        "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
        "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
        "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
        "        out['p4'] = self.pool4(out['r44'])\n",
        "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
        "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
        "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
        "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
        "        out['p5'] = self.pool5(out['r54'])\n",
        "        return [out[key] for key in out_keys]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4NBnp5DOmu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get network\n",
        "vgg = VGG()\n",
        "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "if torch.cuda.is_available():\n",
        "    vgg.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LzbOZRsOc0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gram matrix and loss\n",
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        b,c,h,w = input.size()\n",
        "        F = input.view(b, c, h*w)\n",
        "        G = torch.bmm(F, F.transpose(1,2)) \n",
        "        G.div_(h*w)\n",
        "        return G\n",
        "\n",
        "class GramMSELoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
        "        return(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ0uwTBmVVhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre and post processing for images\n",
        "img_size = 512 \n",
        "prep = transforms.Compose([transforms.Resize(img_size,img_size),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
        "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
        "                          ])\n",
        "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
        "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
        "                           ])\n",
        "postpb = transforms.Compose([transforms.ToPILImage()])\n",
        "def postp(tensor): # to clip results in the range [0,1]\n",
        "    t = postpa(tensor)\n",
        "    t[t>1] = 1    \n",
        "    t[t<0] = 0\n",
        "    img = postpb(t)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw3ZMN61OkTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load input image:\n",
        "input_img_name = 'wall1003.png'#'bark1001.png'\n",
        "input_img = Image.open(image_dir+input_img_name)\n",
        "input_img_torch = Variable(prep(input_img).unsqueeze(0).cuda())\n",
        "\n",
        "#random init:\n",
        "opt_img = Variable(torch.randn(input_img_torch.size()).type_as(input_img_torch.data), requires_grad=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Dp4QgUWVxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 30))\n",
        "axes[0].imshow(postp(input_img_torch.data[0].cpu().squeeze()))\n",
        "axes[0].set_title('original image')\n",
        "axes[1].imshow(postp(opt_img.data[0].cpu().squeeze()))\n",
        "axes[1].set_title('random initialization')\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S_AElbYZmx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define layers, loss functions, weights and compute optimization targets\n",
        "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
        "loss_weights = [1/n**2 for n in [64,128,256,512,512]] #these are good weights settings\n",
        "# Note that weights are equal to number of coefficients in the Gram matrices\n",
        "loss_fns = [GramMSELoss()] * len(style_layers)\n",
        "loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
        "    \n",
        "\n",
        "#compute optimization targets\n",
        "style_targets = [GramMatrix()(A).detach() for A in vgg(input_img_torch, style_layers)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUlVRdIzZ_5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run texture synthesis\n",
        "max_iter = 500\n",
        "show_iter = 50\n",
        "optimizer = optim.LBFGS([opt_img]);\n",
        "n_iter=[0]\n",
        "\n",
        "while n_iter[0] <= max_iter:\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        out = vgg(opt_img, style_layers)\n",
        "        layer_losses = [loss_weights[a] * loss_fns[a](A, style_targets[a]) for a,A in enumerate(out)]\n",
        "        loss = sum(layer_losses)\n",
        "        loss.backward()\n",
        "        n_iter[0]+=1\n",
        "        #print loss\n",
        "        if n_iter[0]%show_iter == (show_iter-1):\n",
        "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.data))\n",
        "#             print([loss_layers[li] + ': ' +  str(l.data[0]) for li,l in enumerate(layer_losses)]) #loss of each layer\n",
        "            #display result\n",
        "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 20))\n",
        "            axes[0].imshow(postp(input_img_torch.data[0].cpu().squeeze()))\n",
        "            axes[0].set_title('original image')\n",
        "            axes[1].imshow(postp(opt_img.data[0].cpu().squeeze()))\n",
        "            axes[1].set_title('synthesis')\n",
        "            fig.tight_layout()\n",
        "            plt.pause(0.05)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    optimizer.step(closure)\n",
        "    \n",
        "#display result\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 30))\n",
        "axes[0].imshow(postp(input_img_torch.data[0].cpu().squeeze()))\n",
        "axes[0].set_title('original image')\n",
        "axes[1].imshow(postp(opt_img.data[0].cpu().squeeze()))\n",
        "axes[1].set_title('synthesis')\n",
        "fig.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}