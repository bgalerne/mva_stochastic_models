{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gatys_texture_synthesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGqt6VdHhEVLJaDzBGQgo4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/mva_stochastic_models/blob/master/Gatys_texture_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsmsJQTTLet_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dDSmyxtLxLk",
        "colab_type": "text"
      },
      "source": [
        "This practical session is based on\n",
        "**[Texture Synthesis Using Convolutional Neural Networks](https://papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks)**\n",
        "by Leon Gatys, Alexander S. Ecker, and Matthias Bethge, NIPS 2015\n",
        "\n",
        "Most of the code is from L. Gatys' repo\n",
        "https://github.com/leongatys/PytorchNeuralStyleTransfer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URrdAlxELyA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os \n",
        "image_dir = os.getcwd()+'/'\n",
        "model_dir = os.getcwd()+'/'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k9FXffLOrDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get binary files for vgg19 parameters:\n",
        "!wget -c --no-check-certificate https://bethgelab.org/media/uploads/pytorch_models/vgg_conv.pth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxSnoBXaTpLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get some texture images (images are 512x512)\n",
        "!wget -c https://www.idpoisson.fr/galerne/mva/bark1001.png\n",
        "!wget -c https://www.idpoisson.fr/galerne/mva/wall1003.png\n",
        "!wget -c https://www.idpoisson.fr/galerne/mva/wall1029.png\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI9G0rDPOVRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vgg definition that conveniently let's you grab the outputs from any layer\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, pool='max'):\n",
        "        super(VGG, self).__init__()\n",
        "        #vgg modules\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        if pool == 'max':\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pool == 'avg':\n",
        "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            \n",
        "    def forward(self, x, out_keys):\n",
        "        out = {}\n",
        "        out['r11'] = F.relu(self.conv1_1(x))\n",
        "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
        "        out['p1'] = self.pool1(out['r12'])\n",
        "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
        "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
        "        out['p2'] = self.pool2(out['r22'])\n",
        "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
        "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
        "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
        "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
        "        out['p3'] = self.pool3(out['r34'])\n",
        "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
        "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
        "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
        "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
        "        out['p4'] = self.pool4(out['r44'])\n",
        "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
        "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
        "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
        "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
        "        out['p5'] = self.pool5(out['r54'])\n",
        "        return [out[key] for key in out_keys]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4NBnp5DOmu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get network\n",
        "vgg = VGG()\n",
        "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "if torch.cuda.is_available():\n",
        "    vgg.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LzbOZRsOc0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gram matrix and loss\n",
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        b,c,h,w = input.size()\n",
        "        F = input.view(b, c, h*w)\n",
        "        G = torch.bmm(F, F.transpose(1,2)) \n",
        "        G.div_(h*w)\n",
        "        return G\n",
        "\n",
        "class GramMSELoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
        "        return(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ0uwTBmVVhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre and post processing for images\n",
        "img_size = 512 \n",
        "prep = transforms.Compose([transforms.Resize(img_size,img_size),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
        "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
        "                          ])\n",
        "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
        "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
        "                           ])\n",
        "postpb = transforms.Compose([transforms.ToPILImage()])\n",
        "def postp(tensor): # to clip results in the range [0,1]\n",
        "    t = postpa(tensor)\n",
        "    t[t>1] = 1    \n",
        "    t[t<0] = 0\n",
        "    img = postpb(t)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw3ZMN61OkTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load input image:\n",
        "input_img_name = 'bark1001.png'\n",
        "input_img = Image.open(image_dir+input_img_name)\n",
        "input_img_torch = Variable(prep(input_img).unsqueeze(0).cuda())\n",
        "\n",
        "#random init:\n",
        "opt_img = Variable(torch.randn(input_img_torch.size()).type_as(input_img_torch.data), requires_grad=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Dp4QgUWVxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 30))\n",
        "axes[0].imshow(postp(input_img_torch.data[0].cpu().squeeze()))\n",
        "axes[0].set_title('original image')\n",
        "axes[1].imshow(postp(opt_img.data[0].cpu().squeeze()))\n",
        "axes[1].set_title('random initialization')\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S_AElbYZmx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define layers, loss functions, weights and compute optimization targets\n",
        "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
        "loss_weights = [1e3/n**2 for n in [64,128,256,512,512]] #these are good weights settings\n",
        "loss_fns = [GramMSELoss()] * len(style_layers)\n",
        "loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
        "    \n",
        "\n",
        "#compute optimization targets\n",
        "style_targets = [GramMatrix()(A).detach() for A in vgg(input_img_torch, style_layers)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUlVRdIzZ_5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run texture synthesis\n",
        "max_iter = 500\n",
        "show_iter = 50\n",
        "optimizer = optim.LBFGS([opt_img]);\n",
        "n_iter=[0]\n",
        "\n",
        "while n_iter[0] <= max_iter:\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        out = vgg(opt_img, style_layers)\n",
        "        layer_losses = [loss_weights[a] * loss_fns[a](A, style_targets[a]) for a,A in enumerate(out)]\n",
        "        loss = sum(layer_losses)\n",
        "        loss.backward()\n",
        "        n_iter[0]+=1\n",
        "        #print loss\n",
        "        if n_iter[0]%show_iter == (show_iter-1):\n",
        "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.data))\n",
        "#             print([loss_layers[li] + ': ' +  str(l.data[0]) for li,l in enumerate(layer_losses)]) #loss of each layer\n",
        "            #display result\n",
        "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 20))\n",
        "            axes[0].imshow(postp(input_img_torch.data[0].cpu().squeeze()))\n",
        "            axes[0].set_title('original image')\n",
        "            axes[1].imshow(postp(opt_img.data[0].cpu().squeeze()))\n",
        "            axes[1].set_title('synthesis')\n",
        "            fig.tight_layout()\n",
        "            plt.pause(0.05)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    optimizer.step(closure)\n",
        "    \n",
        "#display result\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 30))\n",
        "axes[0].imshow(postp(input_img_torch.data[0].cpu().squeeze()))\n",
        "axes[0].set_title('original image')\n",
        "axes[1].imshow(postp(opt_img.data[0].cpu().squeeze()))\n",
        "axes[1].set_title('synthesis')\n",
        "fig.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOMIhpAdLyd5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}